{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb563b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius\n",
    "from torch_geometric.typing import WITH_TORCH_CLUSTER\n",
    "\n",
    "if not WITH_TORCH_CLUSTER:\n",
    "    quit(\"This example requires 'torch-cluster'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc7235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 95, Validation dataset size: 5\n"
     ]
    }
   ],
   "source": [
    "# data set imports\n",
    "import dataset_utils as du\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "B = 1 \n",
    "\n",
    "# load dataset\n",
    "dataset = du.SDFDataset(\"./cars100\")\n",
    "train_percent = 0.95 \n",
    "\n",
    "# split dataset into training and validation sets\n",
    "train_size = int(train_percent * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=B, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7ffb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, np_in=1024, np_q=1024):\n",
    "    B, N, _ = batch.shape\n",
    "    assert B == 1, \"Batch size must be 1 for now.\"\n",
    "    \n",
    "    x = None # ! no features for now\n",
    "    pos = batch[:, :np_in, :3]\n",
    "    \n",
    "    idx = torch.randperm(N)[:np_q]\n",
    "    query_pos = batch[:, idx, :3] # query positions\n",
    "    query_sdf = batch[:, idx, 3] # SDF values\n",
    "    batch_vec = torch.tensor([1]) # batch indices (only have batch_size of 1 for now)\n",
    "    \n",
    "    return x, pos.contiguous(), batch_vec.contiguous(), query_pos, query_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cb988",
   "metadata": {},
   "source": [
    "Define Layers and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0185a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAModule(torch.nn.Module):\n",
    "    def __init__(self, ratio, r, nn):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.r = r\n",
    "        self.conv = PointNetConv(nn, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        idx = fps(pos, batch, ratio=self.ratio)\n",
    "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
    "                          max_num_neighbors=64)\n",
    "        edge_index = torch.stack([col, row], dim=0)\n",
    "        x_dst = None if x is None else x[idx]\n",
    "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
    "        pos, batch = pos[idx], batch[idx]\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class GlobalSAModule(torch.nn.Module):\n",
    "    def __init__(self, nn):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        x = self.nn(torch.cat([x, pos], dim=1))\n",
    "        x = global_max_pool(x, batch)\n",
    "        pos = pos.new_zeros((x.size(0), 3))\n",
    "        batch = torch.arange(x.size(0), device=batch.device)\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input channels account for both `pos` and node features.\n",
    "        self.sa1_module = SAModule(0.5, 0.2, MLP([3, 64, 64, 128]))\n",
    "        self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
    "        self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "        self.encode = MLP([1024, 512, 256], dropout=0.5, norm=None)\n",
    "        self.sdf = MLP([256 + 3, 128, 64, 1], dropout=0.5, norm=None)\n",
    "\n",
    "    def forward(self, x, pos, batch, query_pos):\n",
    "        # encode shape\n",
    "        sa0_out = (x, pos, batch)\n",
    "        sa1_out = self.sa1_module(*sa0_out)\n",
    "        sa2_out = self.sa2_module(*sa1_out)\n",
    "        sa3_out = self.sa3_module(*sa2_out)\n",
    "        x, pos, batch = sa3_out\n",
    "        \n",
    "        x = self.encode(x)\n",
    "        x = torch.cat((x.unsqueeze(1).repeat(1, query_pos.shape[1], 1), query_pos), dim=-1) # concatenate encoded shape with query positions\n",
    "\n",
    "        return self.sdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdada9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# # make random tensor of shape (B, 256, 3) \n",
    "# x = torch.randn(B, 256)\n",
    "# queries = torch.randn(B, 1024, 3) # queries for SDF values\n",
    "\n",
    "# # want to make a tensor of shape (B, 1024, 256 + 3) where the first 256 columns are the x values and the last 3 columns are the query positions\n",
    "# x = torch.cat((x.unsqueeze(1).repeat(1, 1024, 1), queries), dim=-1)  # (B, 1024, 256 + 3)\n",
    "# mlp = MLP([256 + 3, 128, 64, 1], dropout=0.5, norm=None)  # MLP for SDF prediction\n",
    "# x = mlp(x)  # (B, 1024, 1)\n",
    "\n",
    "# print(f\"x shape: {x.shape}\")\n",
    "# print(f\"queries shape: {queries.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b957818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 50000, 4])\n",
      "pos shape: torch.Size([1, 1024, 3]), query_pos shape: torch.Size([1, 1024, 3]), query_sdf shape: torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f'Batch shape: {batch.shape}')\n",
    "x, pos, batch, query_pos, query_sdf = process_batch(batch)\n",
    "print(f'pos shape: {pos.shape}, query_pos shape: {query_pos.shape}, query_sdf shape: {query_sdf.shape}')\n",
    "\n",
    "# print(f'pos shape: {pos.shape}')\n",
    "# # forward pass\n",
    "# output = model(x, pos, batch, query_pos)\n",
    "# # print output shape\n",
    "# print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d934ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m     path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(osp\u001b[38;5;241m.\u001b[39mdirname(osp\u001b[38;5;241m.\u001b[39mrealpath(\u001b[38;5;18m__file__\u001b[39m)), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ModelNet10\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m     pre_transform, transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mNormalizeScale(), T\u001b[38;5;241m.\u001b[39mSamplePoints(\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     17\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m ModelNet(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, transform, pre_transform)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, pos, batch, query_pos, query_sdf = process_batch(batch)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, pos, batch, query_pos).squeeze(-1)\n",
    "        loss = F.mse_loss(out, query_sdf)  # L1 loss for SDF prediction\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    print('Loss: {:.4f}'.format(train(epoch)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
