{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb563b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, PointNetConv, fps, global_max_pool, radius\n",
    "from torch_geometric.typing import WITH_TORCH_CLUSTER\n",
    "\n",
    "if not WITH_TORCH_CLUSTER:\n",
    "    quit(\"This example requires 'torch-cluster'\")\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fc7235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 95, Validation dataset size: 5\n"
     ]
    }
   ],
   "source": [
    "# data set imports\n",
    "import dataset_utils as du\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "B = 1 \n",
    "\n",
    "# load dataset\n",
    "dataset = du.SDFDataset(\"./cars100\")\n",
    "train_percent = 0.95 \n",
    "\n",
    "# split dataset into training and validation sets\n",
    "train_size = int(train_percent * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=B, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f7ffb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, np_in=1024, np_q=1024):\n",
    "    B, N, _ = batch.shape\n",
    "    assert B == 1, \"Batch size must be 1 for now.\"\n",
    "    \n",
    "    x = None # ! no features for now\n",
    "    pos = batch[:, :np_in, :3] # input positions (B, N, 3) -> (N, 3) # this is what fps expects\n",
    "    \n",
    "    idx = torch.randperm(N)[:np_q]\n",
    "    query_pos = batch[:, idx, :3] # query positions\n",
    "    query_sdf = batch[:, idx, 3] # SDF values\n",
    "    \n",
    "    batch_vec = torch.zeros(np_in, dtype=torch.long) # batch vector for input points\n",
    "    \n",
    "    return x, pos.contiguous().squeeze(0), batch_vec.contiguous().squeeze(0), query_pos.squeeze(0), query_sdf.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432cb988",
   "metadata": {},
   "source": [
    "Define Layers and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0185a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAModule(torch.nn.Module):\n",
    "    def __init__(self, ratio, r, nn):\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "        self.r = r\n",
    "        self.conv = PointNetConv(nn, add_self_loops=False)\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        idx = fps(pos, batch, ratio=self.ratio)\n",
    "        row, col = radius(pos, pos[idx], self.r, batch, batch[idx],\n",
    "                          max_num_neighbors=64)\n",
    "        edge_index = torch.stack([col, row], dim=0)\n",
    "        x_dst = None if x is None else x[idx]\n",
    "        x = self.conv((x, x_dst), (pos, pos[idx]), edge_index)\n",
    "        pos, batch = pos[idx], batch[idx]\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class GlobalSAModule(torch.nn.Module):\n",
    "    def __init__(self, nn):\n",
    "        super().__init__()\n",
    "        self.nn = nn\n",
    "\n",
    "    def forward(self, x, pos, batch):\n",
    "        x = self.nn(torch.cat([x, pos], dim=1))\n",
    "        x = global_max_pool(x, batch)\n",
    "        pos = pos.new_zeros((x.size(0), 3))\n",
    "        batch = torch.arange(x.size(0), device=batch.device)\n",
    "        return x, pos, batch\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Input channels account for both `pos` and node features.\n",
    "        self.sa1_module = SAModule(0.5, 0.2, MLP([3, 64, 64, 128]))\n",
    "        self.sa2_module = SAModule(0.25, 0.4, MLP([128 + 3, 128, 128, 256]))\n",
    "        self.sa3_module = GlobalSAModule(MLP([256 + 3, 256, 512, 1024]))\n",
    "\n",
    "        self.encode = MLP([1024, 512, 256], dropout=0.5, norm=None)\n",
    "        self.sdf = MLP([256 + 3, 128, 64, 1], dropout=0.5, norm=None)\n",
    "\n",
    "    def forward(self, x, pos, batch, query_pos):\n",
    "        # encode shape\n",
    "        sa0_out = (x, pos, batch)\n",
    "        sa1_out = self.sa1_module(*sa0_out)\n",
    "        sa2_out = self.sa2_module(*sa1_out)\n",
    "        sa3_out = self.sa3_module(*sa2_out)\n",
    "        x, pos, batch = sa3_out\n",
    "        \n",
    "        x = self.encode(x)\n",
    "        x = torch.cat((x.squeeze(0).repeat(1024, 1), query_pos), dim=-1) # concatenate encoded shape with query positions\n",
    "\n",
    "        return self.sdf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdada9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([1024, 1])\n",
      "queries shape: 3\n"
     ]
    }
   ],
   "source": [
    "# # test\n",
    "# # make random tensor of shape (256, 3) \n",
    "# x = torch.randn(1, 256)\n",
    "# queries = torch.randn(1024, 3) # queries for SDF values\n",
    "\n",
    "# # want to make a tensor of shape (1024, 256 + 3) where the first 256 columns are the x values and the last 3 columns are the query positions\n",
    "# x = torch.cat((x.squeeze(0).repeat(1024, 1), queries), dim=-1)  # (1024, 256 + 3)\n",
    "# mlp = MLP([256 + 3, 128, 64, 1], dropout=0.5, norm=None)  # MLP for SDF prediction\n",
    "# x = mlp(x)  # (B, 1024, 1)\n",
    "\n",
    "# print(f\"x shape: {x.shape}\")\n",
    "# print(f\"queries shape: {queries.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b957818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([1, 50000, 4])\n",
      "pos shape: torch.Size([1024, 3]), query_pos shape: torch.Size([1024, 3]), query_sdf shape: torch.Size([1024])\n",
      "pos shape: torch.Size([1024, 3])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Output shape: torch.Size([1024, 1])\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(f'Batch shape: {batch.shape}')\n",
    "x, pos, batch, query_pos, query_sdf = process_batch(batch)\n",
    "print(f'pos shape: {pos.shape}, query_pos shape: {query_pos.shape}, query_sdf shape: {query_sdf.shape}')\n",
    "\n",
    "print(f'pos shape: {pos.shape}')\n",
    "# forward pass\n",
    "output = model(x, pos, batch, query_pos)\n",
    "# print output shape\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b667f085",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prediction = torch.cat((query_pos, output), dim=-1).detach()  # (1024, 3 + 1)\n",
    "truth = torch.cat((query_pos, query_sdf.unsqueeze(-1)), dim=-1).detach()  # (1024, 3 + 1)\n",
    "\n",
    "du.visualize_sdf_3d(prediction)\n",
    "du.visualize_sdf_2d(prediction)\n",
    "print(\"MODEL\")\n",
    "du.visualize_sdf_3d(truth)\n",
    "du.visualize_sdf_2d(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01d934ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n",
      "Encoded shape: torch.Size([1, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:16<01:06, 16.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:20<01:23, 20.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train(epoch)))\n",
      "Cell \u001b[1;32mIn[56], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epoch):\n\u001b[0;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      8\u001b[0m         x, pos, batch, query_pos, query_sdf \u001b[38;5;241m=\u001b[39m process_batch(batch)\n\u001b[0;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:895\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    893\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 895\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[38;5;241m.\u001b[39mkeys(), timeout)\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:827\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    825\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 827\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    829\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        x, pos, batch, query_pos, query_sdf = process_batch(batch)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, pos, batch, query_pos).squeeze(-1)\n",
    "        loss = F.mse_loss(out, query_sdf)  # L1 loss for SDF prediction\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    print('Loss: {:.4f}'.format(train(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fa7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
