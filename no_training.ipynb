{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4285feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# neuralop imports\n",
    "from neuralop.layers.gno_block import GNOBlock\n",
    "from neuralop.layers.channel_mlp import ChannelMLP\n",
    "from neuralop.layers.fno_block import FNOBlocks\n",
    "\n",
    "# data set imports\n",
    "import dataset_utils as du\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31effef",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4804866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 80, Validation dataset size: 20\n"
     ]
    }
   ],
   "source": [
    "B = 1 # must use batch size of 1 for GNOBlock\n",
    "\n",
    "# load dataset\n",
    "dataset = du.SDFDataset(\"./cars100\")\n",
    "train_percent = 0.8\n",
    "\n",
    "# split dataset into training and validation sets\n",
    "train_size = int(train_percent * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=B, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2552dbd",
   "metadata": {},
   "source": [
    "Define Useful Functions for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e997d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports shape: torch.Size([50000, 3]), Latent queries shape: torch.Size([64, 64, 64, 3]), Features shape: torch.Size([50000, 1]), Output queries shape: torch.Size([1000, 3]), Output labels shape: torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "def process_batch(batch, grid_size, query_size):\n",
    "    # y\n",
    "    input_geom = batch[:, :, :3]  # x, y, z coordinates\n",
    "    input_geom = input_geom.squeeze(0) \n",
    "\n",
    "    # x (grid points in 3D space) generate 64x64x64 grid with bounds [-1, 1] in each dimension\n",
    "    coords = torch.linspace(-1.0, 1.0, grid_size) \n",
    "    x, y, z = torch.meshgrid(coords, coords, coords, indexing='ij') \n",
    "    latent_queries = torch.stack((x, y, z), dim=-1)\n",
    "    # transform to match batch size\n",
    "    latent_queries = latent_queries.repeat(B, 1, 1, 1, 1)  # Repeat for batch size B\n",
    "    latent_queries = latent_queries.squeeze(0) \n",
    "\n",
    "    # f_y\n",
    "    features = batch[:, :, 3]  # features (e.g., colors, normals)\n",
    "    features = features.unsqueeze(-1).squeeze(0)  \n",
    "\n",
    "    # queries (for now just the same as input_geom)\n",
    "    output_queries = input_geom.clone().squeeze()[:query_size,:]  # !For now, just use first 1000 points of input_geom as output_queries\n",
    "    output_labels = features.clone()[:query_size,:]  # !For now, just use first 1000 points of features as output_labels\n",
    "    print(f'Supports shape: {input_geom.shape}, Latent queries shape: {latent_queries.shape}, Features shape: {features.shape}, Output queries shape: {output_queries.shape}, Output labels shape: {output_labels.shape}')\n",
    "    return input_geom, latent_queries, features, output_queries, output_labels\n",
    "\n",
    "# test\n",
    "batch = next(iter(train_loader))\n",
    "input_geom, latent_queries, features, output_queries, output_labels = process_batch(batch, grid_size=64, query_size=1000)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3208fe",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ccb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDFGNO(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=1,\n",
    "                 coord_dim=3,\n",
    "                 out_channels=10,\n",
    "                 lifting_channels=16,\n",
    "                 fno_hidden=32,\n",
    "                 proj_channels=16,\n",
    "                 fno_layers=4,\n",
    "                 fno_modes=(16,16,16),\n",
    "                 grid_size=5,\n",
    "                 query_size=1000):\n",
    "        super().__init__()\n",
    "        radius = 1.0 / grid_size\n",
    "        \n",
    "        # encoder GNO\n",
    "        self.gno_in = GNOBlock(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               coord_dim=coord_dim,\n",
    "                               radius=radius)\n",
    "\n",
    "        # lift into FNO space\n",
    "        self.lifting = ChannelMLP(in_channels=out_channels,\n",
    "                                  hidden_channels=lifting_channels,\n",
    "                                  out_channels=fno_hidden,\n",
    "                                  n_layers=2)\n",
    "\n",
    "        # a stack of Fourier layers\n",
    "        self.fno_blocks = FNOBlocks(in_channels=fno_hidden,\n",
    "                                    out_channels=fno_hidden,\n",
    "                                    n_modes=fno_modes,\n",
    "                                    n_layers=fno_layers)\n",
    "\n",
    "        # decoder GNO\n",
    "        self.gno_out = GNOBlock(in_channels=fno_hidden,\n",
    "                                out_channels=1,\n",
    "                                coord_dim=coord_dim,\n",
    "                                radius=radius)\n",
    "\n",
    "        # final projection to SDF scalar\n",
    "        self.projection = ChannelMLP(in_channels=fno_hidden,\n",
    "                                     hidden_channels=proj_channels,\n",
    "                                     out_channels=1,\n",
    "                                     n_layers=2)\n",
    "\n",
    "        # store grid_size for forward\n",
    "        self.grid_size = grid_size\n",
    "        self.query_size = query_size\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        batch: Tensor of shape [1, N, 4]  (x,y,z, sdf_true)\n",
    "        returns: Tensor of shape [Q, 4]  query (x,y,z,sdf)\n",
    "        \"\"\"\n",
    "        B, N, _ = batch.shape\n",
    "        assert B == 1, \"GNOBlock requires batch size = 1\"\n",
    "        \n",
    "        # process batch to get input_geom, latent_queries, features, output_queries\n",
    "        input_geom, latent_queries, features, output_queries, output_labels = process_batch(batch, self.grid_size, self.query_size)\n",
    "        \n",
    "        \"\"\" GNOBlock Encoding \"\"\"\n",
    "        reshaped_queries = latent_queries.view((-1, latent_queries.shape[-1]))  # Reshape for GNOBlock input\n",
    "        in_p = self.gno_in(y=input_geom, x=reshaped_queries, f_y=features)\n",
    "        # reshape\n",
    "        grid_shape = latent_queries.shape[:-1] # disregard positional encoding dim\n",
    "        in_p = in_p.view((B, *grid_shape, -1)) # add batch\n",
    "\n",
    "        \"\"\" Lifting to FNO latent space \"\"\" # Input shape after lifting: torch.Size([1, FNO_HIDDEN_CHANNELS, 10, 10, 10])\n",
    "        # reshape\n",
    "        in_p = in_p.permute(0, len(in_p.shape)-1, *list(range(1,len(in_p.shape)-1)))\n",
    "        in_p = self.lifting(in_p)\n",
    "\n",
    "        \"\"\" Latent Embedding with FNO Blocks \"\"\" # Input shape after FNOBlocks: torch.Size([1, 10, 10, 10,  FNO_HIDDEN_CHANNELS])\n",
    "        for idx in range(self.fno_blocks.n_layers):\n",
    "            in_p = self.fno_blocks(in_p, idx)\n",
    "        latent_embed = in_p.permute(0, 2, 3, 4, 1)  # Reshape to [B, grid_size, grid_size, grid_size, FNO_HIDDEN_CHANNELS]\n",
    "        \n",
    "        \"\"\" GNOBlock Decoding \"\"\" \n",
    "        reshaped_embed = latent_embed.view((-1, latent_embed.shape[-1]))  # Reshape for GNOBlock input\n",
    "        out = self.gno_out(y=reshaped_queries, x=output_queries, f_y=reshaped_embed)\n",
    "\n",
    "        \"\"\" Projection \"\"\"\n",
    "        out = out.unsqueeze(0).permute(0, 2, 1)  # Reshape to [B, num_queries, FNO_HIDDEN_CHANNELS] for projection\n",
    "        out = self.projection(out)\n",
    "       \n",
    "        # convert to SDF values\n",
    "        out = out.squeeze(0).permute(1, 0)  # Reshape to [num_queries, B] for SDF output\n",
    "        out = torch.cat((output_queries, out), dim=1)  # Concatenate along the feature dimension\n",
    "        \n",
    "        return out, output_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c0fdf",
   "metadata": {},
   "source": [
    "Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfbd9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()  # Mean Squared Error loss for SDF regression\n",
    "\n",
    "def compute_loss(predictions, labels):\n",
    "    # Extract the SDF values from predictions and targets\n",
    "    pred_sdf = predictions[:, 3]  # Assuming the SDF is in the last column\n",
    "    \n",
    "    return loss_fn(pred_sdf, labels)  # Compute MSE loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44c754",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f687bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batches:   0%|          | 0/80 [00:05<?, ?it/s]\n",
      "Training Epochs:   0%|          | 0/1 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     10\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     12\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:895\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    893\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 895\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[38;5;241m.\u001b[39mkeys(), timeout)\n\u001b[0;32m    896\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\anaconda3\\Lib\\multiprocessing\\connection.py:827\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    825\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 827\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    829\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SDFGNO().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=\"Training Batches\"):\n",
    "        batch = batch.to(device)  # Move batch to device\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions, labels = model(batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(predictions, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad95d6",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4733c7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports shape: torch.Size([50000, 3]), Latent queries shape: torch.Size([5, 5, 5, 3]), Features shape: torch.Size([50000, 1]), Output queries shape: torch.Size([100, 3]), Output labels shape: torch.Size([100, 1])\n",
      "Warning: use_scatter is True but torch_scatter is not properly built.                   Defaulting to naive PyTorch implementation\n",
      "Warning: use_scatter is True but torch_scatter is not properly built.                   Defaulting to naive PyTorch implementation\n",
      "TRUE\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_batch = next(iter(val_loader))\n",
    "test_batch = test_batch.to(device)  # Move batch to device\n",
    "predictions, labels = model(test_batch)\n",
    "\n",
    "print(\"TRUE\")\n",
    "du.visualize_sdf_3d(test_batch.squeeze(0))\n",
    "du.visualize_sdf_2d(test_batch.squeeze(0))\n",
    "print(\"MODEL\")\n",
    "du.visualize_sdf_3d(predictions)\n",
    "du.visualize_sdf_2d(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fe786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
