{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272e136d",
   "metadata": {},
   "source": [
    "# **Load Data**\n",
    "\n",
    "This notebook is meant to go through a workflow of:\n",
    "1) Downloading ShapeNet data from Hugging Face\n",
    "2) Processing data (generating SDF examples)\n",
    "3) Uploading data to a Hugging Face repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b282be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general utilities\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# basic imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# working with pointclouds\n",
    "import point_cloud_utils as pcu\n",
    "\n",
    "# ml\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define category IDs (these can be found on the ShapeNet Hugging Face dataset)\n",
    "CANS = \"02946921\"\n",
    "CARS = \"02691156\"\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a70035",
   "metadata": {},
   "source": [
    "**Download Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "TOKEN = \"YOUR_HUGGING_FACE_ACCESS_TOKEN\" # ! Change this to your token\n",
    "CATEGORY_ID = CARS\n",
    "folder_name = \"shapenet_cars\" # create a folder to store your data before running\n",
    "\n",
    "login(token=TOKEN)\n",
    "\n",
    "# download\n",
    "hf_hub_download(\n",
    "    repo_id=\"ShapeNet/ShapeNetCore\",\n",
    "    filename=\"{}.zip\".format(CATEGORY_ID),  # This is the category ID for cars\n",
    "    repo_type=\"dataset\",\n",
    "    token = TOKEN,\n",
    "    local_dir=\"./{}\".format(folder_name),\n",
    "\n",
    ")\n",
    "\n",
    "# extract from zip\n",
    "zip_path = os.path.join(folder_name, \"{}.zip\".format(CATEGORY_ID))\n",
    "extract_dir = folder_name\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204de72",
   "metadata": {},
   "source": [
    "**Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:: 100%|██████████| 70/70 [03:32<00:00,  3.03s/it]\n"
     ]
    }
   ],
   "source": [
    "CATEGORY_ID = CARS\n",
    "N_MODELS = 70 #None - option to process smaller subset\n",
    "\n",
    "relative_data_path = \"shapenet_cars\"\n",
    "category_path = os.path.join(os.getcwd(), relative_data_path, CATEGORY_ID)\n",
    "out_path = os.path.join(os.getcwd(), relative_data_path, \"out\")\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "# Resolution used to convert shapes to watertight manifolds\n",
    "manifold_resolution = 20_000 # Higher value means better quality and slower\n",
    "\n",
    "# Number of points in the volume to sample around the shape\n",
    "num_vol_pts = 30_000\n",
    "\n",
    "# Number of points on the surface to sample\n",
    "num_surf_pts = 10_000\n",
    "\n",
    "# Number of points near the surface to sample\n",
    "num_near_surf_pts = 10_000\n",
    "sigma = 0.05\n",
    "\n",
    "\n",
    "iterable = os.listdir(category_path)[:N_MODELS] if N_MODELS is not None else os.listdir(category_path)\n",
    "for i, model_path in enumerate(tqdm(iterable, desc=\"Processing models:\")):\n",
    "\n",
    "    mesh_path = os.path.normpath(os.path.join(category_path, model_path, \"models\", \"model_normalized.obj\"))\n",
    "\n",
    "    # Load mesh\n",
    "    v, f = pcu.load_mesh_vf(mesh_path) # load object\n",
    "    # Convert mesh to watertight manifold\n",
    "    vm, fm = pcu.make_mesh_watertight(v, f, manifold_resolution)\n",
    "\n",
    "    \"\"\" Volume Sampling \"\"\"\n",
    "    # Generate random points in the volume around the shape\n",
    "    p_vol = (np.random.rand(num_vol_pts, 3) - 0.5) * 1.2 # NOTE: ShapeNet shapes are normalized within [-0.5, 0.5]^3\n",
    "\n",
    "    # Compute the SDF of the random points\n",
    "    sdf_vol, _, _  = pcu.signed_distance_to_mesh(p_vol, vm, fm)\n",
    "    \n",
    "    # Generate data matrix with coordinates and sdf_vol values in format xyzs\n",
    "    data_vol = np.concatenate((p_vol, sdf_vol.reshape(-1, 1)), axis=1)\n",
    "\n",
    "    \"\"\" Surface Sampling \"\"\"\n",
    "    # Sample points on the surface as face ids and barycentric coordinates\n",
    "    fid_surf, bc_surf = pcu.sample_mesh_random(vm, fm, num_surf_pts)\n",
    "\n",
    "    # Compute 3D coordinates of surface samples\n",
    "    p_surf = pcu.interpolate_barycentric_coords(fm, fid_surf, bc_surf, vm)\n",
    "    \n",
    "    # Generate data matrix with sdf values in format xyzs (where sdf value = 0)\n",
    "    data_surf = np.concatenate((p_surf, np.zeros((num_surf_pts, 1))), axis=1)\n",
    "\n",
    "    \"\"\" Near Surface Sampling \"\"\"\n",
    "    # Sample points on the surface as face ids and barycentric coordinates\n",
    "    fid_near_surf, bc_near_surf = pcu.sample_mesh_random(vm, fm, num_near_surf_pts)\n",
    "\n",
    "    # Compute 3D coordinates of surface samples\n",
    "    p_near_surf = pcu.interpolate_barycentric_coords(fm, fid_near_surf, bc_near_surf, vm)\n",
    "    noise = np.random.normal(loc=0.0, scale=sigma, size=p_near_surf.shape)\n",
    "    p_near_surf = p_near_surf + noise\n",
    "\n",
    "    # Compute the SDF of the near surface points\n",
    "    sdf_near_surf, _, _  = pcu.signed_distance_to_mesh(p_near_surf, vm, fm)\n",
    "    \n",
    "    # Generate data matrix with coordinates and sdf_near_surf values in format xyzs\n",
    "    data_near_surf = np.concatenate((p_near_surf, sdf_near_surf.reshape(-1, 1)), axis=1)\n",
    "    \n",
    "    \"\"\" Saving \"\"\"\n",
    "    # Concatenate surface and volume data\n",
    "    data = np.concatenate((data_vol, data_surf, data_near_surf), axis=0)\n",
    "    data = torch.from_numpy(data).float()\n",
    "    \n",
    "    # Save data to file\n",
    "    torch.save(data, os.path.normpath(os.path.join(out_path, \"{}.pt\".format(model_path))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb918f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped folder saved to: /vast/palmer/home.mccleary/cpsc452_lrk42/cpsc452/shapenet_cars/out.zip\n"
     ]
    }
   ],
   "source": [
    "# zip the out folder\n",
    "zip_filename = os.path.join(os.getcwd(), relative_data_path, \"out.zip\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(out_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Write file with relative path inside the zip\n",
    "            arcname = os.path.relpath(file_path, start=out_path)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(f\"Zipped folder saved to: {zip_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3afdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f3b4a540cc49948d5969692f3adee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "out.zip:   0%|          | 0.00/50.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/lukasskellijs/carssdf/commit/0f680a13418ef565f4d240e8efd2ea794d3ea7d8', commit_message='Upload cars70.zip with huggingface_hub', commit_description='', oid='0f680a13418ef565f4d240e8efd2ea794d3ea7d8', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/lukasskellijs/carssdf', endpoint='https://huggingface.co', repo_type='dataset', repo_id='lukasskellijs/carssdf'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "from huggingface_hub import HfApi, HfFolder, upload_folder, upload_file\n",
    "\n",
    "# set dataset parameters\n",
    "reponame = \"carssdf\"\n",
    "username = \"YOUR_HUGGING_FACE_USERNAME\"\n",
    "repo_id = \"{}/{}\".format(username, reponame)\n",
    "\n",
    "create_repo(repo_id, repo_type=\"dataset\", private=True)\n",
    "\n",
    "# Upload all contents of the out folder\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=out_path,\n",
    "    path_in_repo=\"\",  # Optional: subfolder inside the repo\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# upload zipped out file\n",
    "upload_file(\n",
    "    path_or_fileobj=zip_filename,\n",
    "    path_in_repo=\"out.zip\",\n",
    "    repo_id=\"{}/{}\".format(username, reponame),\n",
    "    repo_type=\"dataset\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
