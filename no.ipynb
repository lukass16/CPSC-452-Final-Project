{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae1c7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import torch\n",
    "\n",
    "# neuralop imports\n",
    "from neuralop.layers.gno_block import GNOBlock\n",
    "from neuralop.layers.channel_mlp import ChannelMLP\n",
    "from neuralop.layers.fno_block import FNOBlocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141a3e6",
   "metadata": {},
   "source": [
    "Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afadd264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 5 Radius: 0.2\n",
      "Batch shape: torch.Size([1, 50000, 4])\n"
     ]
    }
   ],
   "source": [
    "import dataset_utils \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "B = 1\n",
    "train_percent = 0.8\n",
    "grid_size = 5\n",
    "radius = 1 / grid_size\n",
    "print(\"Grid size:\", grid_size, \"Radius:\", radius)\n",
    "\n",
    "# load dataset\n",
    "dataset = dataset_utils.SDFDataset(\"./cars100\")\n",
    "\n",
    "# split dataset into training and validation sets\n",
    "train_size = int(train_percent * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create data loaders for training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=B, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Get 1 Batch\n",
    "batch = next(iter(train_loader))\n",
    "batch = batch[:, :, :] # Limit to 5000 points\n",
    "print(\"Batch shape:\", batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcdec0d",
   "metadata": {},
   "source": [
    "Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a1e0449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supports shape: torch.Size([50000, 3]), Latent queries shape: torch.Size([5, 5, 5, 3]), Features shape: torch.Size([50000, 1])\n"
     ]
    }
   ],
   "source": [
    "# y\n",
    "input_geom = batch[:, :, :3]  # x, y, z coordinates\n",
    "input_geom = input_geom.squeeze(0) # ! unbatch\n",
    "\n",
    "# x (grid points in 3D space) generate 64x64x64 grid with bounds [-1, 1] in each dimension\n",
    "coords = torch.linspace(-1.0, 1.0, grid_size) \n",
    "x, y, z = torch.meshgrid(coords, coords, coords, indexing='ij') \n",
    "latent_queries = torch.stack((x, y, z), dim=-1)\n",
    "# transform to match batch size\n",
    "latent_queries = latent_queries.repeat(B, 1, 1, 1, 1)  # Repeat for batch size B\n",
    "latent_queries = latent_queries.squeeze(0)  # ! unbatch\n",
    "\n",
    "# f_y\n",
    "features = batch[:, :, 3]  # features (e.g., colors, normals)\n",
    "features = features.unsqueeze(-1).squeeze(0)  # ! unbatch\n",
    "\n",
    "\n",
    "print(f'Supports shape: {input_geom.shape}, Latent queries shape: {latent_queries.shape}, Features shape: {features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8df5b0",
   "metadata": {},
   "source": [
    "Set up Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12ca3fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNOBlock(\n",
      "  (pos_embedding): SinusoidalEmbedding()\n",
      "  (neighbor_search): NeighborSearch()\n",
      "  (channel_mlp): LinearChannelMLP(\n",
      "    (fcs): ModuleList(\n",
      "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (3): Linear(in_features=128, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (integral_transform): IntegralTransform(\n",
      "    (channel_mlp): LinearChannelMLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Linear(in_features=384, out_features=128, bias=True)\n",
      "        (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (3): Linear(in_features=128, out_features=10, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "FNOBlocks(\n",
      "  (convs): ModuleList(\n",
      "    (0-3): 4 x SpectralConv(\n",
      "      (weight): DenseTensor(shape=torch.Size([32, 32, 16, 16, 9]), rank=None)\n",
      "    )\n",
      "  )\n",
      "  (fno_skips): ModuleList(\n",
      "    (0-3): 4 x Flattened1dConv(\n",
      "      (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (channel_mlp): ModuleList(\n",
      "    (0-3): 4 x ChannelMLP(\n",
      "      (fcs): ModuleList(\n",
      "        (0): Conv1d(32, 16, kernel_size=(1,), stride=(1,))\n",
      "        (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (channel_mlp_skips): ModuleList(\n",
      "    (0-3): 4 x SoftGating()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 10 # latent embedding from GNO encoder\n",
    "COORD_DIM = 3\n",
    "LIFTING_CHANNELS = 16 # from paper\n",
    "FNO_HIDDEN_CHANNELS = 32 # from paper\n",
    "\n",
    "FNO_N_LAYERS = 4 # from paper  (# number of FNO layers in the FNOBlocks)\n",
    "fno_n_modes=(16, 16, 16) # from paper, number of Fourier modes in each dimension \n",
    "\n",
    "\"\"\" GNO Block\"\"\"\n",
    "gno_in = GNOBlock(\n",
    "    in_channels=IN_CHANNELS,\n",
    "    out_channels=OUT_CHANNELS,\n",
    "    coord_dim=COORD_DIM,\n",
    "    radius=radius\n",
    "    )\n",
    "\n",
    "print(gno_in)\n",
    "\n",
    "\n",
    "\"\"\" Lifting \"\"\"\n",
    "# takes per-grid point features from GNO encoder and projects them into FNOs latent channel space \n",
    "lifting = ChannelMLP(in_channels=OUT_CHANNELS, \n",
    "                     hidden_channels=LIFTING_CHANNELS, \n",
    "                     out_channels=FNO_HIDDEN_CHANNELS, \n",
    "                     n_layers=2) \n",
    "\n",
    "\n",
    "\"\"\" FNO Blocks \"\"\"\n",
    "fno_blocks = FNOBlocks(in_channels=FNO_HIDDEN_CHANNELS, out_channels=FNO_HIDDEN_CHANNELS, n_modes=fno_n_modes, n_layers=FNO_N_LAYERS)\n",
    "print(fno_blocks)\n",
    "def latent_embedding(in_p):\n",
    "    for idx in range(fno_blocks.n_layers):\n",
    "        in_p = fno_blocks(in_p, idx)\n",
    "\n",
    "    return in_p \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ef1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped queries shape: torch.Size([125, 3])\n",
      "Warning: use_scatter is True but torch_scatter is not properly built.                   Defaulting to naive PyTorch implementation\n",
      "Latent embedding shape: torch.Size([1, 5, 5, 5, 32])\n",
      "Flattened latent embedding shape: torch.Size([1, 125, 32])\n"
     ]
    }
   ],
   "source": [
    "reshaped_queries = latent_queries.view((-1, latent_queries.shape[-1]))  # Reshape for GNOBlock input\n",
    "print(f'Reshaped queries shape: {reshaped_queries.shape}')\n",
    "\n",
    "\"\"\" GNOBlock Encoding \"\"\" # Input shape after GNOBlock: torch.Size([1, 10, 10, 10, OUTchannels])\n",
    "in_p = gno_in(y=input_geom, x=reshaped_queries, f_y=features)\n",
    "\n",
    "# reshape\n",
    "grid_shape = latent_queries.shape[:-1] # disregard positional encoding dim\n",
    "in_p = in_p.view((B, *grid_shape, -1)) # add batch\n",
    "\n",
    "\n",
    "\"\"\" Lifting to FNO latent space \"\"\" # Input shape after lifting: torch.Size([1, FNO_HIDDEN_CHANNELS, 10, 10, 10])\n",
    "# reshape\n",
    "in_p = in_p.permute(0, len(in_p.shape)-1, *list(range(1,len(in_p.shape)-1)))\n",
    "in_p = lifting(in_p)\n",
    "\n",
    "\"\"\" Latent Embedding with FNO Blocks \"\"\" # Input shape after FNOBlocks: torch.Size([1, 10, 10, 10,  FNO_HIDDEN_CHANNELS])\n",
    "latent_embed = latent_embedding(in_p)\n",
    "latent_embed = latent_embed.permute(0, 2, 3, 4, 1)  # Reshape to [B, grid_size, grid_size, grid_size, FNO_HIDDEN_CHANNELS]\n",
    "print(f'Latent embedding shape: {latent_embed.shape}')\n",
    "\n",
    "# flatten embeddings to [batch_size, grid_size**3, FNO_HIDDEN_CHANNELS]\n",
    "latent_embed = latent_embed.view(B, -1, FNO_HIDDEN_CHANNELS)  # Flatten to [B, grid_size**3, FNO_HIDDEN_CHANNELS]\n",
    "print(f'Flattened latent embedding shape: {latent_embed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079657f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52c3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
